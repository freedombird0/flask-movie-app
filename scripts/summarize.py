import os
import torch
from transformers import pipeline

# Verify the correct processing unit is being used
device = "cuda" if torch.cuda.is_available() else "cpu"
device_id = 0 if device == "cuda" else -1
print(f"âœ… Using device: {device.upper()} (device_id={device_id})")

# Load the summarization model
try:
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=device_id)
except Exception as e:
    print(f"âŒ Error loading summarization model: {e}")
    exit()

# ğŸ”¹ Define paths
base_folder = "C:\\Users\\Mahdi1\\Desktop\\mov"
input_file = os.path.join(base_folder, "output", "cleaned_text.txt")  # Clean text file
output_file = os.path.join(base_folder, "output", "summary.txt")

# ğŸ”¹ Verify that the correct file exists
if not os.path.exists(input_file):
    print("âŒ File cleaned_text.txt is missing! Make sure to run clean_text.py first.")
    exit()

# ğŸ”¹ Read the text from the file
with open(input_file, "r", encoding="utf-8") as f:
    text = f.read().strip()

# ğŸ”¹ Check if the text is empty
if not text or len(text.split()) < 50:  
    print("âŒ The file is empty or the text is too short! Nothing to summarize.")
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("âš ï¸ Insufficient content to summarize.\n")
    exit()

print("ğŸ”„ Summarizing text...")

# ğŸ”¹ Function to split text into smaller chunks for the model
def chunk_text(text, max_tokens=400):
    words = text.split()
    chunks = [" ".join(words[i:i+max_tokens]) for i in range(0, len(words), max_tokens)]
    return [chunk for chunk in chunks if len(chunk.split()) > 40]  # Avoid empty or too-small chunks

# ğŸ”¹ Split the text into chunks suitable for summarization
chunks = chunk_text(text)

# ğŸ”¹ Ensure there is content after splitting
if not chunks:
    print("âŒ Error: No sufficient text chunks found after splitting.")
    exit()

print(f"ğŸ”¹ Text split into {len(chunks)} chunk(s)")

# ğŸ”¹ Use batched processing
batch_size = min(2, len(chunks))  # Process in batches
summarized_text = ""

for i in range(0, len(chunks), batch_size):
    batch = chunks[i:i+batch_size]

    # Verify that each batch has enough data
    if not all(batch) or any(len(chunk.strip()) < 40 for chunk in batch):
        print(f"âš ï¸ Skipping empty or too-short batch {i//batch_size + 1}")
        continue

    print(f"ğŸ”¹ Processing batch {i//batch_size + 1} of {len(chunks)//batch_size + 1}...")

    try:
        summaries = summarizer(
            batch, 
            max_length=120,  # Maximum words in each summary
            min_length=50,   # Minimum words
            do_sample=False
        )
        summarized_text += "\n".join([s["summary_text"] for s in summaries]) + "\n"

    except Exception as e:
        print(f"âš ï¸ Error summarizing batch {i//batch_size + 1}: {e}")
        summarized_text += "[âš ï¸ Summarization error]\n"

# ğŸ”¹ Save the summary
with open(output_file, "w", encoding="utf-8") as f:
    f.write(summarized_text.strip())

print(f"âœ… Summary saved to: {output_file}")
print(f"ğŸ“„ Word count in summary: {len(summarized_text.split())}")

# ğŸ”¹ Display the first 100 words of the summary for verification
preview_text = " ".join(summarized_text.split()[:100])
print(f"\nğŸ“Œ First 100 words of summary:\n{preview_text if preview_text else 'ğŸ”´ Summarization failed!'}")
